# -*- coding: utf-8 -*-
"""ist718_prj_part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YjbMJDZbv8Bu4ywxK3NecPeKtD8ng4hX
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install pyspark &> /dev/null

"""#### Code Author: Danila Rozhevskii"""

# import statements here
# create spark and sparkcontext objects
from pyspark.sql import SparkSession
from pyspark.sql import Row
import numpy as np
import pandas as pd
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

import pandas as pd
import datetime as dt
import matplotlib.pyplot as plt
from matplotlib import pyplot as plt
from matplotlib import rcParams
import seaborn as sns
import re

# import the training data for the CLIP
# in the future i plan to use the classification model form part 1 (Naive Bayes)
# to predict the categories.

# the dataset i use here has categories, but i wanted to use the predicted ones and see whether i can improve
# the fine-tuned CLIP model with that data.

# u can change the data paths since i will provide the data i used with the project
datasource = pd.read_csv('/content/drive/MyDrive/train_images/Mind-Body-Spirit/Mind-Body-Spirit.csv', delimiter=',')

# the validation data i use to test the CLIP model and try out some prompts and see how well fine-tuned CLIP
# will be able to recognize them
datasource_val = pd.read_csv('/content/drive/MyDrive/train_images/Medical/Medical.csv', delimiter=',')

from google.colab import drive
drive.mount('/content/drive')

datasource.head(5)

datasource.shape

#plot value counts of team in descending order
datasource_val.name.value_counts()[:10].plot(kind='bar', rot=90)

"""###Train Data"""

# here i prepare the training data for fine-tuning CLIP on
# in the future I plan to precict the categories for each book title first

data_med = datasource.loc[datasource['category'] == 'Mind-Body-Spirit']
data_med.head(5)

data_med.shape

# since im using data from the drive, I want to change the image path for the CLIP to be able to read the image files for training
data_med['img_paths'] = data_med['img_paths'].str.replace('dataset/Mind-Body-Spirit/', '')
data_med.head(5)

df = data_med[['img_paths', 'name', 'author']]
df.head()

# i want to put all the image paths in a separate list
image_paths = ["/content/drive/MyDrive/train_images/Mind-Body-Spirit/"+i for i in df['img_paths']]
len(image_paths)

# image captions are also put in a separate list

# in the future i will include the predicted categories within each caption
captions = [i for i in df['name']]
len(captions)

# this is final training dataset that i will be feeding into a training dataloader function

train_data = pd.DataFrame(list(zip(captions, image_paths)),columns=['caption','image_url'])
train_data = train_data.applymap(str)
train_data

"""#### I perform all the same steps for the validation data

###Validation data
"""

data_val = datasource_val.loc[datasource_val['category'] == 'Health']
data_val.head(5)

data_val['img_paths'] = data_val['img_paths'].str.replace('dataset/Health/', '')
data_val.head(5)

df = data_val[['img_paths', 'name', 'author']][:10]
df.head()

image_paths_val = ["/content/drive/MyDrive/train_images/Health/"+i for i in df['img_paths']]
len(image_paths_val)

captions_val = [i for i in df['name']]
len(captions_val)

val_data = pd.DataFrame(list(zip(captions_val, image_paths_val)),columns=['caption','image_url'])
val_data = val_data.applymap(str)
val_data

"""## MODEL Algoirthm"""

!pip install sparktorch

! pip install ftfy regex tqdm
! pip install git+https://github.com/openai/CLIP.git

! pip install transformers

from pkg_resources import packaging
print("Torch version:", torch.__version__)

from sparktorch import serialize_torch_obj, SparkTorch
import torch
import torch.nn as nn
from pyspark.ml.feature import VectorAssembler
from pyspark.sql import SparkSession
from pyspark.ml.pipeline import Pipeline

import requests
import clip
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
from PIL import Image
from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel

from tqdm.autonotebook import tqdm

!nvcc --version

torch.cuda.is_available()

# i define the CUDA device and make sure GPUs are available for the fine-tuning
# I also import the 32bit pre-trained CLIP model that I will try fine-tuning

device = "cuda:0" if torch.cuda.is_available() else "cpu" # If using GPU then use mixed precision training.
model, preprocess = clip.load("ViT-B/32",device=device,jit=False) #Must set jit=False for training

train_data.head(5)

"""###Torch distriubutor"""

#!pip install sparktorch
from pyspark.ml.torch.distributor import TorchDistributor

# the class below is written with help from: https://github.com/openai/CLIP/issues/83
# it accepts training data as input and performs text and image vectorization using
# internal CLIP funcions to transform both caption and image columns in the training data
BATCH_SIZE = 32

class image_title_dataset(Dataset):
    def __init__(self, list_image_path,list_txt):

        self.image_path = list_image_path
        self.title  = clip.tokenize(list_txt)

    def __len__(self):
        return len(self.title)

    def __getitem__(self, idx):
        image = preprocess(Image.open(self.image_path[idx])) # Image from PIL module
        title = self.title[idx]
        return image,title

# i define the function that uses the class to create a training dataloader
def load_features(list_image_path, list_txt):
  dataset = image_title_dataset(list_image_path,list_txt)
  train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE)
  return train_dataloader

#Define torch distributor object for the dataloader
train_dataloader = TorchDistributor(num_processes=1,
local_mode=True,
use_gpu=False).run(load_features,
train_data["image_url"].values, train_data["caption"].values)

# fucntion to convert the model to f32
def convert_models_to_fp32(model):
    for p in model.parameters():
        p.data = p.data.float()
        p.grad.data = p.grad.data.float()

# define parameters for the training
loss_img = nn.CrossEntropyLoss()
loss_txt = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params from paper

# the train fucntion
# train function is also wirtten with the help from: https://github.com/openai/CLIP/issues/83
# it takes the images and captions from the training dataloader and creates logits, which are probabilities of similarities

def train_clip(EPOCH, train_dataloader):
  for epoch in range(EPOCH):
    for batch in train_dataloader:
        optimizer.zero_grad()

        images,texts = batch

        images= images.to(device)
        texts = texts.to(device)

        logits_per_image, logits_per_text = model(images, texts)

        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)

        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2
        total_loss.backward()
        if device == "cpu":
          optimizer.step()

        else :
          convert_models_to_fp32(model)
          optimizer.step()
          clip.model.convert_weights(model)

#Define torch distributor object for the training step
# this will allow to run the fine-tuning process in parallel in case i have multiple nodes i can use

TorchDistributor(num_processes=1,
local_mode=True,
use_gpu=False).run(train_clip,
11, train_dataloader)

# save the model
# i save the model's checkpoint in order ot be able to contintue fine-tuning process in the future
# with the updated data (predicted cateogries included in the caption for exmaple)
torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss,
        }, f"/content/drive/MyDrive/model_test2.pt")

#Load the model at the checkpoint
checkpoint = torch.load("/content/drive/MyDrive/model_test2.pt", map_location=torch.device('cpu'))

model.load_state_dict(checkpoint['model_state_dict'])

image_path = val_data["image_url"][0]
Image.open(image_path)

# Calculating caption probabilities for test image

image = preprocess(Image.open(val_data["image_url"][0])).unsqueeze(0).to(device)
text = clip.tokenize(['Lord of Rings','The Power of Nowww', 'random book', 'Health Power']).to(device)

with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)

    logits_per_image, logits_per_text = model(image, text)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()

print("Label probs:", probs)

# creating a test dataset
testdata = val_data[:10]
testdata.caption

# Calculating similarity scores between images and captions

image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
text_inputs = torch.cat([clip.tokenize(c) for c in testdata.caption]).to(device)

# Calculate features
with torch.no_grad():
    image_features = model.encode_image(image_input)
    text_features = model.encode_text(text_inputs)

# Pick the top 5 most similar labels for the image
image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
values, indices = similarity[0].topk(5)

# Print the result
print("\nTop predictions:\n")
for value, index in zip(values, indices):
    print(f"{index}: {100 * value.item():.2f}%")

"""#### This is the same training process as above, but written in PyTorch only without TorhcDistributor

###Pytorch
"""

BATCH_SIZE = 32

class image_title_dataset(Dataset):
    def __init__(self, list_image_path,list_txt):

        self.image_path = list_image_path
        self.title  = clip.tokenize(list_txt) #you can tokenize everything at once in here(slow at the beginning), or tokenize it in the training loop.

    def __len__(self):
        return len(self.title)

    def __getitem__(self, idx):
        image = preprocess(Image.open(self.image_path[idx])) # Image from PIL module
        title = self.title[idx]
        return image,title

# use your own data
list_image_path = train_data["image_url"].values
list_txt = train_data["caption"].values
dataset = image_title_dataset(list_image_path,list_txt)
train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE) #Define your own dataloader

val_data["caption"][0]

def convert_models_to_fp32(model):
    for p in model.parameters():
        p.data = p.data.float()
        p.grad.data = p.grad.data.float()

Image.open('/content/drive/MyDrive/train_images/Medical/0000001.jpg')

loss_img = nn.CrossEntropyLoss()
loss_txt = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params from paper

EPOCH = 11

for epoch in range(EPOCH):
  for batch in train_dataloader:
      optimizer.zero_grad()

      images,texts = batch

      images= images.to(device)
      texts = texts.to(device)

      logits_per_image, logits_per_text = model(images, texts)

      ground_truth = torch.arange(len(images),dtype=torch.long,device=device)

      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2
      total_loss.backward()
      if device == "cpu":
         optimizer.step()

      else :
        convert_models_to_fp32(model)
        optimizer.step()
        clip.model.convert_weights(model)

torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss,
        }, f"/content/drive/MyDrive/model_test1.pt") #just change to your preferred folder/filename

model, preprocess = clip.load("ViT-B/32",device=device,jit=False) #Must set jit=False for training
checkpoint = torch.load("/content/drive/MyDrive/model_test1.pt", map_location=torch.device('cpu'))

model.load_state_dict(checkpoint['model_state_dict'])

image_path = val_data["image_url"][0]
Image.open(image_path)

val_data["caption"][0]

#testdata = train_data.sample(n=10)
testdata = val_data[:10]
testdata.caption

"""### Image finding best fit

#### First experiment
"""

# here i test the capability of fine-tuned model
# i input the example image from validation set as well as a set of captions
# and i want to see how each caption is similar to the example image

image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
text_inputs = torch.cat([clip.tokenize(c) for c in testdata.caption]).to(device)
#text_inputs = torch.cat(["Random Book","The Power of Cringe", "Cat Stories"]).to(device)

# Calculate features
with torch.no_grad():
    image_features = model.encode_image(image_input)
    text_features = model.encode_text(text_inputs)

# Pick the top 5 most similar labels for the image
image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
values, indices = similarity[0].topk(5)

# Print the result
# since the first title prompt refers to the title of the exmaple image it has the highest accuracy
print("\nTop predictions:\n")
for value, index in zip(values, indices):
    print(f"{index}: {100 * value.item():.2f}%")

"""#### Second experiment"""

Image.open(testdata["image_url"][0])

# here i want to give CLIP a set of random prompts as well as one closely related to the example book
# i want to see if CLIP can recognize the closest title to the book cover

image = preprocess(Image.open(val_data["image_url"][0])).unsqueeze(0).to(device)
text = clip.tokenize(['Lord of Rings','The Power of Nowww', 'random book', 'Health Power']).to(device)
#text = clip.tokenize([c for c in testdata.caption]).to(device)

with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)

    logits_per_image, logits_per_text = model(image, text)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()

print("Label probs:", probs)

"""#### The resutls show that second title prompit i gave the model is the closest and others are pretty close to 0, which makes sense. This shows that the model is able to assosiate the title prompt and give it the best book cover estimate.

#### In the future, I plan to add a predicted category that can be used with the title prompt and it may increase the accuracy of the fine-tuned CLIP.
"""
