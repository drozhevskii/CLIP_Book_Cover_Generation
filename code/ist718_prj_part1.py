# -*- coding: utf-8 -*-
"""ist718_prj_part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jQFrYrBTgWsYJylepZtn_oS3rbck3rsm
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install pyspark &> /dev/null

"""#### Code author: Danila Rozhevskii"""

# import statements here
# create spark and sparkcontext objects
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
from pyspark.sql import Row
import numpy as np
import pandas as pd
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

sqlContext = SQLContext(sc)

import pandas as pd
import datetime as dt
import matplotlib.pyplot as plt
from matplotlib import pyplot as plt
from matplotlib import rcParams
import seaborn as sns
import re

from pyspark.sql.functions import *

"""###Data import"""

datasource = spark.read.format("csv") \
                  .load('/content/book32-listing.csv', delimiter=',', encoding='ISO-8859-1')

newColumns = ['ASIN', 'FILENAME', 'IMAGE_URL', 'TITLE', 'AUTHOR', 'CATEGORY_ID', 'CATEGORY']
df = datasource.toDF(*newColumns)
df.printSchema()

df.show(5)

"""###Preprocess"""

# i drop all the NA values from the dataset
df_noNull = df.na.drop()

print((df_noNull.count(), len(df_noNull.columns)))

# add a DESCR column which combines authot and title to make the best descriptive parameter for book title
df_noNull = df_noNull.withColumn("DESCR", concat(df_noNull.TITLE,df_noNull.AUTHOR))

df2 = df_noNull.select(df_noNull.columns[3:8])
df2.show(5)

#Top 20 book categories:
df2.groupBy("CATEGORY") \
    .count() \
    .orderBy(col("count").desc()) \
    .show()

#Top 20 authors:
df2.groupBy("AUTHOR") \
    .count() \
    .orderBy(col("count").desc()) \
    .show()

#Top 20 titles:
df2.groupBy("DESCR") \
    .count() \
    .orderBy(col("count").desc()) \
    .show()

"""###Tokenize, create vectors"""

# i create a few tokenizers for every column (title, author separately) as well as for DESCR column.
# i create stopwordsRemover for Title and DESCR, since there are no stopwords in Author column.
# i create a few count vectorizers for each tokenizer
# i experimented with different values for the vocabSize and minDF for the countVectorizer and 30,000 and 10 were yielding the best results
# i also use the assembler to create single vector combining author and title

from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression

# regular expression tokenizer
regexTokenizer1 = RegexTokenizer(inputCol="TITLE", outputCol="TITLEwords", pattern="\\W")
regexTokenizer2 = RegexTokenizer(inputCol="AUTHOR", outputCol="AUTHORwords", pattern="\\W")
regexTokenizer3 = RegexTokenizer(inputCol="DESCR", outputCol="DESCRwords", pattern="\\W")

# stop words
add_stopwords = ["and","a","the"]
stopwordsRemover = StopWordsRemover(inputCol="TITLEwords", outputCol="TITLEfiltered").setStopWords(add_stopwords)
stopwordsRemover3 = StopWordsRemover(inputCol="DESCRwords", outputCol="DESCRfiltered").setStopWords(add_stopwords)

# bag of words count
countVectors1 = CountVectorizer(inputCol="TITLEfiltered", outputCol="features1", vocabSize=30000, minDF=10)
countVectors2 = CountVectorizer(inputCol="AUTHORwords", outputCol="features2", vocabSize=30000, minDF=10)
countVectors3 = CountVectorizer(inputCol="DESCRfiltered", outputCol="features", vocabSize=30000, minDF=10)

#assembler = VectorAssembler(inputCols=["vectorizedFeatures1","vectorizedFeatures2"], outputCol='features')

assembler = VectorAssembler(inputCols=["features1","features2"], outputCol='features')

#frequency label with StringIndexer
from pyspark.ml.feature import StringIndexer
label_stringIdx = StringIndexer(inputCol='CATEGORY',outputCol='label').fit(df)

from pyspark.ml import Pipeline
from pyspark.ml.feature import OneHotEncoder, VectorAssembler

# two pipelinens are created in case some models break
# cross validation on logistic regression for some reason was working with pipeline2 but not with pipeline1.

pipeline1 = Pipeline(stages=[regexTokenizer1, regexTokenizer2, stopwordsRemover, countVectors1, countVectors2, assembler, label_stringIdx])
#pipeline2 = Pipeline(stages=[regexTokenizer3, stopwordsRemover3, countVectors3, label_stringIdx])

# Fit the pipeline to training data.
pipelineFit = pipeline1.fit(df2)
dataset = pipelineFit.transform(df2)
dataset.show(5)

"""###Partition Training & Test sets"""

# set seed for reproducibility
(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)
print("Training Dataset Count: " + str(trainingData.count()))
print("Test Dataset Count: " + str(testData.count()))

"""###Model Training and Evaluation"""

#Logistic Regression using Count Vector Features

lr = LogisticRegression(maxIter=100, regParam=0.3, elasticNetParam=0)
lrModel = lr.fit(trainingData)

predictions = lrModel.transform(testData)
predictions.filter(predictions['prediction'] == 0) \
    .select("TITLE","AUTHOR","CATEGORY","probability","label","prediction") \
    .orderBy("probability", ascending=False) \
    .show(n = 10, truncate = 30)

# Evaluation
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

evaluator = MulticlassClassificationEvaluator(predictionCol="prediction")
evaluator.evaluate(predictions)

"""###Cross-Validation"""

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(predictionCol="prediction")

# here i use the second pipeline since first one breaks and use single DESCR column as predictor input
pipeline2 = Pipeline(stages=[regexTokenizer3, stopwordsRemover3, countVectors3, label_stringIdx])

pipelineFit = pipeline2.fit(df2)
dataset = pipelineFit.transform(df2)
(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)

lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# Create ParamGrid for Cross Validation
# i experiment with regulazation parameters since the model tends to overfit a lot
paramGrid = (ParamGridBuilder()
             .addGrid(lr.regParam, [0.1, 0.3]) # regularization parameter
             .addGrid(lr.elasticNetParam, [0.0, 0.1]) # Elastic Net Parameter (Ridge = 0)
             .build())

# Create 5-fold CrossValidator
cv = CrossValidator(estimator=lr, \
                    estimatorParamMaps=paramGrid, \
                    evaluator=evaluator, \
                    numFolds=5)
cvModel = cv.fit(trainingData)

predictions = cvModel.transform(testData)
# Evaluate best model
evaluator = MulticlassClassificationEvaluator(predictionCol="prediction")
evaluator.evaluate(predictions)

"""###Naive Bayes"""

from pyspark.ml.classification import NaiveBayes

nb = NaiveBayes(smoothing=1)
model = nb.fit(trainingData)
predictions = model.transform(testData)
predictions.filter(predictions['prediction'] == 0) \
    .select("TITLE","AUTHOR","CATEGORY","probability","label","prediction") \
    .orderBy("probability", ascending=False) \
    .show(n = 10, truncate = 30)

evaluator = MulticlassClassificationEvaluator(predictionCol="prediction")
evaluator.evaluate(predictions)

"""###Random Forest"""

from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline

rf = RandomForestClassifier(labelCol="label", featuresCol="features")
# Train model with Training Data
rfModel = rf.fit(trainingData)
pred = rfModel.transform(testData)

evaluator = MulticlassClassificationEvaluator(predictionCol="prediction")
evaluator.evaluate(pred)

# 3-Fold Cross validation for Random Forest
from pyspark.ml.tuning import ParamGridBuilder
from pyspark.ml.tuning import CrossValidator
from pyspark.ml.evaluation import RegressionEvaluator
import numpy as np

# i was trying to make this part work but I couldnt, and my teammates did not help me unfortunately
paramGrid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [int(x) for x in np.linspace(start = 10, stop = 100, num = 3)]) \
    .addGrid(rf.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 25, num = 3)]) \
    .build()

crossval = CrossValidator(estimator=rf,
                          estimatorParamMaps=paramGrid,
                          evaluator=RegressionEvaluator(),
                          numFolds=3)

cvModel = crossval.fit(trainingData)

predictions = cvModel.transform(testData)
# Evaluate best model
evaluator = MulticlassClassificationEvaluator(predictionCol="prediction")
evaluator.evaluate(predictions)

